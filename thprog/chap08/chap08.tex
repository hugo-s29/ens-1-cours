\documentclass[../main]{subfiles}

\begin{document}
  \chapter{Un petit langage impératif, $\mathsf{IMP}$.} \label{thprog-chap08}
  \minitoc

  \section{Syntaxe et sémantique opérationnelle.}
  On se donne $\mathds{Z}$ et $\mathrm{V}$ un ensemble infini de variables $\mathsf{IMP}$, notées~$x, y, z$.
  On définit plusieurs grammaires :

  \begin{description}
    \item[Arith.] Les expressions arithmétiques $a ::= \ubar{k}  \mid a_1 \oplus a_2  \mid x$ ;\footnote{Et on arrêtera rapidement de mettre des barres sous les entiers et d'entourer les plus.}\showfootnote
    \item[Valeurs booléennes.] $bv ::= \mathtt{true}  \mid \mathtt{false}$ ;
    \item[Bool.] Les expressions booléennes $b ::= bv  \mid b_1 \land b_2  \mid a_1 \ge a_2$ ;
    \item[Com.] Les commandes $c ::= x \ceq a  \mid c_1 \col c_2  \mid \ifte b {c_1} {c_2}  \mid \while b c  \mid \mathtt{skip}$.
  \end{description}

  Sans explicitement le dire, on s'autorise à étendre les expressions arithmétiques avec, par exemple, les produits, les soustractions.
  De même pour les expressions booléennes.

  On définit, par induction sur $c$, $\mathsf{Vars}(c)$ l'ensemble des variables dans la commande $c$. Il y a 5 cas.

  \begin{exm}
    La commande \[
      z \ceq 1 \col \while {(x > 0)} {(z \ceq z \times x \col x \ceq x - 1)}
    \]
    représente un programme calculant la factorielle d'un nombre $x$.
    On le notera $c_\text{fact}$.
  \end{exm}

  \subsection{Sémantique opérationnelle à grands pas.}

  \begin{defn}[États mémoire]
    On se donne $\mathcal{M}$ un ensemble de dictionnaires, notés $\sigma, \sigma'$, \textit{etc} sur $(\mathrm{V}, \mathds{Z})$.

    Si $x \in \mathrm{dom}(\sigma)$ et $k \in \mathds{Z}$ on note $\sigma [x \mapsto k]$ l'état mémoire $\sigma'$ défini par 
     \begin{itemize}
      \item $\sigma'(x) := k$ ;
      \item  $\sigma'(y) := \sigma(y)$ si  $y \in \mathrm{dom}(\sigma) \setminus \{x\}$.
    \end{itemize}

    Ici, on \textit{écrase} la valeur de $x$ dans l'état mémoire $\sigma$.
  \end{defn}

  On définit $c, \sigma \Downarrow \sigma'$ (l'évaluation de $c$ sur $\sigma$ produit $\sigma'$, $c$ fait passer de $\sigma$ à $\sigma'$) par les règles d'inférences ci-dessous
  \begin{gather*}
    \begin{prooftree}
      \infer 0 [\mathcal{E}_{\mathrm{skip}}] { \mathtt{skip}, \sigma \Downarrow \sigma}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \hypo {c_1, \sigma \Downarrow \sigma'}
      \hypo {c_2, \sigma' \Downarrow \sigma''}
      \infer 2[\mathcal{E}_\mathrm{seq}] {c_1 \col c_2, \sigma \Downarrow \sigma''}
    \end{prooftree}
  \end{gather*}
  \begin{gather*}
    \begin{prooftree}
      \hypo{\color{deeppurple}b,\sigma \Downarrow \mathtt{true}}
      \hypo{c_1, \sigma \Downarrow \sigma'}
      \infer 2[\mathcal{E}_\mathrm{it}]{\ifte b {c_1} {c_2}, \sigma \Downarrow \sigma'}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \hypo{\color{deeppurple}b,\sigma \Downarrow \mathtt{false}}
      \hypo{c_2, \sigma \Downarrow \sigma'}
      \infer 2[\mathcal{E}_\mathrm{if}]{\ifte b {c_1} {c_2}, \sigma \Downarrow \sigma'}
    \end{prooftree}
  \end{gather*}
  \begin{gather*}
    \begin{prooftree}
      \hypo{{\color{deepblue} a, \sigma \Downarrow k}}
      \infer[left label={\sigma' = \sigma[x \mapsto k]}] 1[\mathcal{E}_\mathrm{aff}] {x \ceq a, \sigma \Downarrow \sigma'}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \hypo{\color{deeppurple}b, \sigma \Downarrow \mathtt{false}}
      \infer 1[\mathcal{E}_\mathrm{wf}]{\while b c, \sigma \Downarrow \sigma}
    \end{prooftree}
  \end{gather*}
  \begin{gather*}
    \begin{prooftree}
      \hypo{\color{deeppurple}b, \sigma \Downarrow \mathtt{true}}
      \hypo{c, \sigma \Downarrow \sigma'}
      \hypo{\while b c, \sigma' \Downarrow \sigma''}
      \infer 3[\mathcal{E}_\mathrm{wf}]{\while b c, \sigma \Downarrow \sigma''}
    \end{prooftree}
  \end{gather*}
  où l'on a deux autres relations (la couleur a de l'importance ici) :
  \begin{itemize}
    \item \textit{l'évaluation des expressions arithmétiques} {\color{deepblue} $a, \sigma \Downarrow k$} ($a$ s'\textit{évalue} en $k$ dans $\sigma$)
      \begin{gather*}
        \begin{prooftree}
          \infer 0{\color{deepblue} \ubar{k}, \sigma \Downarrow k}
        \end{prooftree}
        \quad\quad
        \begin{prooftree}
          \infer[left label={\sigma(x) = k}] 0{\color{deepblue} x, \sigma \Downarrow k}
        \end{prooftree}
        \quad\quad
        \begin{prooftree}
          \hypo{\color{deepblue} a_1, \sigma \Downarrow k_1}
          \hypo{\color{deepblue} a_2, \sigma \Downarrow k_2}
          \infer[left label={k = k_1 + k_2}] 2{\color{deepblue}a_1 \oplus a_2, \sigma \Downarrow k}
        \end{prooftree}
      \end{gather*}
    \item \textit{l'évaluation des expressions booléennes} {\color{deeppurple} $b, \sigma \Downarrow bv$} ($b$ s'\textit{évalue} en $bv$ dans $\sigma$)
      \begin{gather*}
        \begin{prooftree}
          \infer 0{\color{deeppurple}bv, \sigma \Downarrow bv}
        \end{prooftree}
        \quad\quad
        \begin{prooftree}
          \hypo{\color{deeppurple}b_1, \sigma \Downarrow bv_1}
          \hypo{\color{deeppurple}b_2, \sigma \Downarrow bv_2}
          \infer[left label={bv = \mathtt{true} \text{ ssi } bv_1 \text{ et } bv_2}] 2{\color{deeppurple}b_1 \land b_2, \sigma \Downarrow bv}
        \end{prooftree}
      \end{gather*}
      \begin{gather*}
        \begin{prooftree}
          \hypo{\color{deepblue} a_1, \sigma \Downarrow k_1}
          \hypo{\color{deepblue} a_2, \sigma \Downarrow k_2}
          \infer[left label={bv = \mathtt{true} \text{ ssi } k_1 \ge k_2}] 2{\color{deeppurple}a_1 \ge a_2, \sigma \Downarrow bv.}
        \end{prooftree}
      \end{gather*}
  \end{itemize}

  \begin{rmk}[des "variables" partout !]~
    \begin{itemize}
      \item Les variables dans $\mathsf{FUN}$ sont les paramètres des fonctions, elles peuvent être liées, libres, et on peut procéder à de l'$\alpha$-conversion.\footnote{C'est similaire au cas de la variable $x$ dans $\int_0^7 f(x)\;\mathrm{d}x$.}
      \item Les variables d'unification sont des inconnues. Il y a une notion de substitution, mais pas de liaison.
      \item Les variables dans $\mathsf{IMP}$ sont des cases mémoire, des registres, et il n'y a pas de liaison.
    \end{itemize}
  \end{rmk}

  \begin{rmk}
    Soit $c$ une commande, et $\sigma \in \mathcal{M}$.
    Il peut arriver que, quel que soit $\sigma' \in \mathcal{M}$, on n'ait pas $c, \sigma \Downarrow \sigma'$,
    soit parce que $\mathrm{dom}(\sigma)$ est trop petit, et l'exécution se bloque ;
    soit parce que le programme diverge, par exemple \[
      \while {\mathtt{true}} {\mathtt{skip}}
    \]
    diverge car on n'a pas de dérivation finies :
    \[
    \begin{prooftree}
      \hypo{\color{deeppurple} \mathtt{true}, \sigma \Downarrow \mathtt{true}}
      \infer 0{\mathtt{skip}, \sigma \Downarrow \sigma}
      \hypo{\while {\mathtt{true}} {\mathtt{skip}}, \sigma \Downarrow {?}}
      \infer 3[\mathcal{E}_\mathrm{wt}]{\while {\mathtt{true}} {\mathtt{skip}}, \sigma \Downarrow {?}}
    \end{prooftree}
    .\] 
  \end{rmk}

  On peut définir des petits pas pour $\mathsf{IMP}$ (vu plus tard en cours, ou en TD), mais on s'intéresse plus à une autre sémantique, la \textit{sémantique dénotationnelle}.

  \section{Sémantique dénotationnelle de $\mathsf{IMP}$.}

  \begin{figure}[H]
    \centering

    \begin{tikzpicture}
      \node (1) {$c$};
      \node[right=2cm of 1] (2) {fonction partielle $\mathcal{M} \rightharpoonup \mathcal{M}$};
      \node[below=1cm of 2] (3) {~\hspace{1cm}relation binaire sur $\mathcal{M}$ déterministe/fonctionnelle};
      \draw[->] (1) -- (2) node[midway, above] {$\mathcal{D}$};
      \draw[<->] (2) -- (3);
    \end{tikzpicture}
  \end{figure}

  On définit les relations
  \begin{itemize}
    \item ${\color{deepblue}\mathcal{D}(a)} \subseteq \mathcal{M} \times \mathds{Z}$ fonctionnelle ;
    \item ${\color{deeppurple}\mathcal{D}(b)} \subseteq \mathcal{M} \times \{\mathtt{true},\mathtt{false}\}$ fonctionnelle ;
    \item $\mathcal{D}(c) \subseteq \mathcal{M} \times \mathcal{M}$ fonctionnelle.
  \end{itemize}
  On ne traitera que la définition de $\mathcal{D}(c)$, les autres sont laissées en exercice.

  On définit $\mathcal{D}(c)$ par induction sur $c$, il y a 5 cas.
  \begin{itemize}
    \item $\mathcal{D}(\mathtt{skip}) = \{(\sigma, \sigma)\}$ ;
    \item \small$\mathcal{D}(x \ceq a) = \{(\sigma, \sigma')  \mid x \in \mathrm{dom}(\sigma), \sigma' = \sigma[x \mapsto k] \text{ et  } (\sigma, k) \in {\color{deepblue}\mathcal{D}(a)}\}$ ;
    \item \footnotesize $\mathcal{D}(\ifte b {c_1} {c_2}) = \{(\sigma, \sigma'  \mid (\sigma, \mathtt{true}) \in {\color{deeppurple}\mathcal{D}(b)}, (\sigma, \sigma') \in \mathcal{D}(c_1))\}  \cup {}$\\
        $\phantom{\mathcal{D}(\ifte b {c_1} {c_2}) ={}}\{(\sigma, \sigma'  \mid (\sigma, \mathtt{false}) \in {\color{deeppurple}\mathcal{D}(b)}, (\sigma, \sigma') \in \mathcal{D}(c_2))\}$ ;
        \item $\mathcal{D}(c_1 \ceq c_2) = \{(\sigma, \sigma'')  \mid \exists \sigma', (\sigma, \sigma') \in \mathcal{D}(c_1) \text{ et } (\sigma', \sigma'') \in \mathcal{D}(c_2)\}$ ;\footnote{C'est la composée de $\mathcal{D}(c_2)$ avec $\mathcal{D}(c_1)$.}\showfootnote
        \item $\mathcal{D}(\while b c) = {???}$.
  \end{itemize}

  Pour la sémantique dénotationnelle de la boucle \texttt{while}, 
  on s'appuie sur l'"équivalence" des commandes \[
    \while b c \quad\quad\text{et}\quad\quad \ifte b {(c \ceq \while b c)} {\mathtt{skip}}.
  \]
  On introduit, pour $\mathsf{R} \subseteq \mathcal{M} \times \mathcal{M}$, la relation
  \begin{align*}
    F(\mathsf{R}) &{}=\quad \{(\sigma,\sigma)  \mid (\sigma, \mathtt{false}) \in {\color{deeppurple}\mathcal{D}(b)}\} \\
    &\quad{}\cup \{(\sigma, \sigma')  \mid (\sigma, \mathtt{true}) \in {\color{deeppurple}\mathcal{D}(b)}, \exists \sigma', (\sigma, \sigma') \in \mathcal{D}(c) \text{ et } (\sigma', \sigma'') \in \mathsf{R} \} 
  .\end{align*}
  On a envie de définir $\mathcal{D}(\while b c)$ comme un point fixe de $F$.

  L'ensemble des relations binaires fonctionnelles sur $\mathcal{M}$ \textbf{n'est pas} un treillis complet (à cause se $\mathsf{R}_1 \cup \mathsf{R}_2$ qui n'est pas nécessairement fonctionnelle).
  On ne peut donc pas appliquer le théorème de Knaster-Tarski.

  En revanche, c'est un domaine : si $e_0 \subseteq e_1 \subseteq \cdots \subseteq e_n \subseteq \cdots$ alors l'union $\bigcup_{i \ge 0} e_i$ existe.
  L'inclusion $e \subseteq e'$ signifie que $e'$ est "plus définie" que $e$.
  L'ensemble des relations fonctionnelles sur $\mathcal{M}$ est donc un domaine avec $\bot = \emptyset$.
  On sait donc que, pour toute fonction $F$ continue, alors $F$ admet un point fixe, qui est égal à \[
  \emptyset \cup F(\emptyset) \cup  F^2(\emptyset) \cup  \cdots = \bigcup_{i\ge 0} F^i(\emptyset)
  .\]

  La fonction $F$ définie plus haut est continue, ce qui nous permet de définir \[
  \mathcal{D}(\while b c) = \bigcup_{i \ge 0} F^i(\emptyset)
  .\]

  \begin{exm}
    On considère $c_0 = \while {x \neq 3} x \ceq x - 1$.
    Ainsi, la fonction $F$ définie avant $c = c_0$ est
    \begin{align*}
      F_0(\mathsf{R}) &{}= \quad\{(\sigma, \sigma)  \mid \sigma(x) = 3\}\\
      &\quad\cup \{(\sigma, \sigma'')  \mid \sigma(x) \neq 3, \exists \sigma', \sigma = [x \mapsto \sigma(x) - 1], (\sigma,\sigma') \in \mathsf{R}\} 
    .\end{align*}
    On a 
    \begin{itemize}
      \item \footnotesize $F_0^0(\emptyset) = \{(\sigma, \sigma)  \mid \sigma(x) = 3\}$  ;
      \footnotesize\item $F_0^1(\emptyset) = \{(\sigma, \sigma)  \mid \sigma(x) = 3\} \cup \{(\sigma, \sigma')  \mid \sigma' = [x \mapsto 3], \sigma(x) = 4\} $ ;
      \item \footnotesize$F_0^2(\emptyset) = \{(\sigma, \sigma)  \mid \sigma(x) = 3\} \cup \{(\sigma, \sigma')  \mid \sigma' = [x \mapsto 3], \sigma(x) \in \{4,5\} \} $  ;
      \item \footnotesize$F_0^2(\emptyset) = \{(\sigma, \sigma)  \mid \sigma(x) = 3\} \cup \{(\sigma, \sigma')  \mid \sigma' = [x \mapsto 3], \sigma(x) \in \{4,5,6\} \} $ ;
      \item \textit{etc}.
    \end{itemize}
    On a bien \[
    \emptyset\subseteq F_0(\emptyset) \subseteq F^2_0(\emptyset) \subseteq \cdots 
    .\]

    Si $\sigma(x) = 0$, alors quel que soit $\sigma'$, on a $(\sigma, \sigma') \not\in \mathcal{D}(c_0)$.
  \end{exm}

  \begin{exm}
    Ainsi défini, \[
      \mathcal{D}(\while {\mathtt{true}} {\mathtt{skip}}) = \emptyset
    .\]
  \end{exm}

  \begin{thm}
    On a $c, \sigma \Downarrow \sigma'$ si et seulement si $(\sigma, \sigma') \in \mathcal{D}(c)$.
  \end{thm}
  \begin{prv}[Éléments de preuve]
    \begin{itemize}
      \item "$\implies$" Par induction sur la relation $c, \sigma \Downarrow \sigma'$.
      \item "$\impliedby$" Par induction sur $c$,
        où l'on utilise le résultat suivant :
        \[
        \forall n, \quad (\sigma, \sigma') \in F^n(\emptyset) \implies c, \sigma  \Downarrow \sigma'
        .\]
    \end{itemize}
  \end{prv}

  \begin{lem}
    Quels que soient $c, \sigma, \sigma_1$, si $c, \sigma, \sigma_1$ alors, \[
    \forall \sigma_2, \quad c, \sigma \Downarrow \sigma_2 \implies \sigma_1 = \sigma_2
    .\]
  \end{lem}
  \begin{prv}
    Une mauvaise idée est de procéder par induction sur $c$.
    Il y a 5 cas, et dans le cas  \texttt{while}, ça bloque parce que la relation grands pas n'est pas définie par induction sur $c$ dans le cas \texttt{while}.

    On procède par induction sur $c, \sigma \Downarrow \sigma_1$.
  \end{prv}

  De manière générale, avec $\mathsf{IMP}$, on ne montre pas des résultats de la forme $c, \sigma \Downarrow \sigma' \implies \mathcal{P}$ par induction sur $c$, car cela ne fonctionne pas, on n'a pas les bonnes hypothèses.
  On procède par induction sur la relation $c, \sigma \Downarrow \sigma'$.

  \section{Coinduction.}

  On retourne sur le théorème de Knaster-Tarski pour la définition d'ensembles et de relations.
  En notant $E$ l'ensemble ambiant, on travaille dans le treillis complet $(\wp(E), \subseteq)$, avec des fonctions $f$ croissantes dans $\wp(E)$.
  Le théorème de Knaster-Tarski nous donne ainsi le plus petit pré-point fixe de $f$, que l'on notera $\mu f$.
  Le principe de la preuve par induction est ainsi :
  \begin{center}
    si $A \subseteq E$ vérifie $f(A) \subseteq A$ alors on a $\mu f \subseteq A$.
  \end{center}
  De plus, si $f$ est continue (car $(\wp(E), \subseteq)$ est un domaine), alors on peut calculer explicitement ce plus petit (pré)-point fixe avec la formule $\bigcup_{n \in \mathds{N}} f^n(\emptyset)$.
  On part du "bas" et on ajoute des éléments un par un.

  \begin{exm}
    Pour l'exemple de $\mathsf{nat}$, on a \[
    \forall A \subseteq E, \quad\quad f(A) = \{\mathtt{O}\} \cup \{\mathtt{S}\ x  \mid x \in A\}
    ,\]
    c'est une fonction continue, et on a \[
    \mu f = \{\mathtt{S}^n \mathtt{O}  \mid  n\in \mathds{N}\}
    ,\] avec $\mathtt{S}^n\ x = \mathtt{S}\ \mathtt{S}\ \cdots \ \mathtt{S}\ x$ et la convention $\mathtt{S}^0\ x = x$.
    En effet, on a  l'appartenance de $\mathtt{S}^n \mathtt{O} \in \bigcup_{m \in \mathds{N}} f^m(\emptyset)$ et $f(\{\mathtt{S}^n\ \mathtt{O}\}) = \{\mathtt{S}^{n+1}\ \mathtt{O}\}$.
  \end{exm}

  \begin{rmk}[Remarque fondamentale !]
    Considérons un treillis complet $(E, \sqsubseteq)$.
    Alors, le treillis $(E, \sqsupseteq)$ est complet, où l'on note~$y \sqsupseteq x$ dès lors que $x \sqsubseteq y$ (on renverse l'ordre).

    Un majorant pour $\sqsubseteq$ est un minorant pour $\sqsupseteq$ et inversement.
    Ainsi, le plus plus petit des majorants $\bigsqcup_\sqsubseteq A$ pour $\sqsubseteq$ est le plus petit des minorants $\bigsqcap_\sqsupseteq A$ pour $\sqsupseteq$.
    Réciproquement, le plus petit des majorants pour $\sqsubseteq$, $\bigsqcap_{\sqsubseteq} A$ est égal au plus grand majorant pour la relation $\sqsupseteq$, $\bigsqcup_{\sqsupseteq} A$.
  \end{rmk}

  On se place ainsi sur le treillis complet $(\wp(E), \supseteq)$.
  Une fonction est croissante pour $\subseteq$ si et seulement si elle est croissante pour $\supseteq$ (\textit{\textbf{attention}}, elle n'est pas décroissance pour cette deuxième relation).
  Appliquons le théorème de Knaster-Tarski sur ce nouveau treillis complet à une fonction croissante.
  Le théorème nous fournis un pré-point fixe pour l'ordre $\supseteq$ (\textit{i.e.} qui vérifie $f(A) \supseteq A$), c'est-à-dire un post-point fixe pour l'ordre $\subseteq$ (\textit{i.e.} qui vérifie $A \subseteq f(A)$).
  Et, c'est le plus petit point fixe pour $\supseteq$, donc le plus grand point fixe pour $\subseteq$, que l'on notera $\nu f$.

  Avec le théorème de point fixe sur les domaines, et en supposant $f$ continue, on calcule explicitement que le plus grand point fixe $\nu f$ vaut l'intersection $\bigcap_{n \in \mathds{N}} f^n(E)$.
  On part du haut, et on nettoie progressivement, on raffine notre partie de $E$.

  Ce que l'on a fait là, cela s'appelle de la \textit{\textbf{coinduction}}.

  \begin{exm}
    Par exemple, on définit $\mathsf{conat}$ par coinduction.
    En Rocq, cela donne le code ci-dessous.
    \begin{lstlisting}[language=coq,caption=Définition de $\mathsf{conat}$]
  CoInductive $\mathsf{conat}$ : Set := cO | cS (n : $\mathsf{conat}$).
    \end{lstlisting}

    Pour illustrer le "nettoyage" effectué dans la définition coinductive, on considère une feuille étiquetée par le mot "\texttt{banane}".
    A-t-on $\mathtt{cS}\ \mathtt{banane} \in \mathsf{conat}$ ?
    Premièrement, on a $\mathtt{cS}\ \mathtt{banane} \in E$ car $E$ est l'ensemble (très grand) des arbres étiquetés par des chaînes de caractères.
    Deuxièmeement, on a $\mathtt{cS}\ \mathtt{banane} \in f(E)$ car c'est le successeur de $\mathtt{banane} \in E$.
    Troisièmement, et c'est là où ça casse, on a $\mathtt{cS\ banane} \not\in f^2(E)$ parce que $\mathtt{banane} \not\in f(E)$.
    

    Avec la fonction $f$ définie précédemment, on a \[
    f^n (E) = \{\mathtt{cO}, \mathtt{cS}\ \mathtt{cO}, \ldots, \mathtt{cS}^{n-1}\ \mathtt{cO}\} \cup \{\mathtt{cS}^n \ x  \mid  x \in E\} 
    .\]

    Ainsi, on récupère tous les entiers de $\mathsf{nat}$, mais d'autres entiers (oui, il y en a plusieurs) infinis, ayant ainsi une dérivation infinie.
    Par exemple, il existe $\omega \in \mathsf{conat}$ tel que $\omega = \mathtt{cS}\ \omega$.
    En Rocq, pour le définir, on ferai :
    \begin{lstlisting}[language=coq,caption={Définition de $\omega$, un entier infini}]
  CoFixpoint $\omega$ := cS $\omega$
    \end{lstlisting}
    Pour montrer que $\omega \in \mathsf{conat}$, il faut et il suffit de montrer l'inclusion $\{\omega\}\subseteq f(\{\omega\} ) = \{\mathtt{cO}, \mathtt{cS}\ \omega\} = \{\mathtt{cO}, \omega\}$, qui est vraie, et on a ainsi $\{\omega\} \subseteq \mathsf{conat}$.
  \end{exm}

  Le principe de la preuve par coinduction permet d'établir qu'un ensemble est contenu dans le plus grand point fixe.
  Avec le treillis des parties muni de $\subseteq$, cela permet de montrer que $A \subseteq E$ est inclus dans le plus grand post-point fixe de $f$ et, pour cela, il suffit de montrer que $A \subseteq f(A)$, c'est-à-dire que $A$ est un post-point fixe de $f$.
  C'est ce que l'on a fait dans l'exemple avec $\omega$.

  Par coinduction, on peut par exemple montrer que l'on a, pour tous états mémoire $\sigma, \sigma'$, \[
    \while {\mathtt{true}} {\mathtt{skip}}, \sigma \Downarrow \sigma'
  .\] 

  \section{Divergences en $\mathsf{IMP}$.}

  On donne une définition coinductive de la divergence en $\mathsf{IMP}$, que l'on notera $c, \sigma \Uparrow$ avec les règles 
  \begin{gather*}
    \begin{prooftree}
      \hypo{c_1, \sigma \Uparrow}
      \infer 1{c_1 \col c_2, \sigma \Uparrow}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \hypo{c_1, \sigma \Downarrow \sigma'}
      \hypo{c_2, \sigma' \Uparrow}
      \infer 2{c_1 \col c_2, \sigma \Uparrow}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \hypo{b, \sigma \Downarrow \mathtt{true}}
      \hypo{c_1, \sigma \Uparrow}
      \infer 2{\ifte b {c_1} {c_2}, \sigma \Downarrow}
    \end{prooftree}\\[1em]
    \begin{prooftree}
      \hypo{b, \sigma \Downarrow \mathtt{false}}
      \hypo{c_2, \sigma \Uparrow}
      \infer 2{\ifte b {c_1} {c_2}, \sigma \Downarrow}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \hypo{b, \sigma \Downarrow \mathtt{true}}
      \hypo{c, \sigma \Uparrow}
      \infer 2{\while b c, \sigma \Uparrow}
    \end{prooftree}\\[1em]
    \begin{prooftree}
      \hypo{b, \sigma \Downarrow \mathtt{true}}
      \hypo{c, \sigma \Downarrow \sigma'}
      \hypo{\while b c, \sigma' \Uparrow}
      \infer 3{\while b c, \sigma \Uparrow}
    \end{prooftree}
  \end{gather*}

  On n'a pas de règle pour la divergence si $b, \sigma \Downarrow \mathtt{false}$, car dans ce cas là, on ne peut pas diverger (c'est équivalent à un $\mathtt{skip}$).

  Le plus grand point fixe ne contient que des dérivations infinies, qui correspondent à des exécutions divergentes d’un programme $\mathsf{IMP}$ à partir d’un état mémoire donné.
  En effet, ceci vient du fait que, si on interprète ces règles comme des règles inductives, la relation obtenue est l'ensemble vide\ldots

  \section{Logique de Floyd--Hoare.}

  On considère des formules logiques, des \textit{assertions} (définies formellement ci-après), que l'on notera $A$, $A'$, $B$, \textit{etc}.
  Un triplet de Hoare est de la forme $\{A\} c \{A'\}$ (la notation est inhabituelle pour les triplets, mais c'est une notation commune dans le cas des triplets de Hoare), où l'on nomme $A$ la \textit{précondition} et $A'$ la \textit{postcondition}.

  \begin{exm}
    Les triplets suivants sont des triplets de Hoare :
    \begin{enumerate}
      \item $\{x \ge 1\} y \ceq x + 2 \{x\ge 1 \land y \ge 3\}$ qui est une conclusion naturelle ;
      \item $\{n \ge 1\} c_\text{fact} \{r = n!\}$ où l'on note $c_\text{fact}$ la commande \[
          x \ceq n\col z \ceq 1 \col \while {(x > 0)} {(z \ceq z \times x \col x \ceq x - 1)}\;
        ,\]
        qui calcule naturellement la factorielle de $n$ ;
      \item $\{x < 0\} c \{\mathtt{true}\}$ même s'il ne nous dit rien d'intéressant (tout état mémoire vérifie $\mathtt{true}$) ;
      \item $\{x < 0\}c \{\mathtt{false}\}$ qui diverge dès lors que $x < 0$.
    \end{enumerate}
  \end{exm}

  On considère un ensemble $I \ni i$ infini d'\textit{index}, des "inconnues".
  On commence par définir les expressions arithmétiques étendues \[
    a ::= \ubar{k}  \mid a_1 \oplus a_2  \mid x  \mid i
  ,\]
  puis définit les \textit{assertions} par la grammaire ci-dessous :
  \[
    A ::= bv  \mid A_1 \lor A_2  \mid A_1 \land A_2 \mid a_1 \ge a_2  \mid \exists i, A
  .\]

  On s'autorisera à étendre, implicitement, les opérations réalisées dans les expressions arithmétiques, et les comparaisons effectuées dans les assertions.

  On ajoute la liaison d'$\alpha$-conversion : les assertions $\exists i, x = 3 * i$ et~$\exists j, x = 3 * j$ sont $\alpha$-équivalentes.
  On note $i\ell(A)$ l'ensemble des index libres de l'assertion~$A$, et on dira que $A$ est \textit{close} dès lors que~$i\ell(A) = \emptyset$.
  On note aussi $A[\sfrac{k}{i}]$ l'assertion $A$ où $k \in \mathds{Z}$ remplace~$i \in I$.

  \begin{defn}
    Considérons $A$ close et $\sigma \in \mathcal{M}$.
    On définit par induction sur $A$ (4 cas) une relation constituée de couples $(\sigma, A)$, notés  $\sigma \models A$ ("$\sigma$ satisfait $A$"), et en notant $\sigma \not\models A$ lorsque~$(\sigma, A)$ n'est pas dans la relation :
    \begin{itemize}
      \item $\sigma \models \mathtt{true}$ $\forall \sigma \in \mathcal{M}$ ;
      \item $\sigma \models A_1 \lor A_2$ si et seulement si $\sigma \models A_1$ ou $\sigma \models A_2$ ;
      \item $\sigma \models a_1 \ge a_2$ si et seulement si on a $a_1, \sigma \Downarrow k_1$ et $a_2, \sigma \Downarrow k_2$ et $k_1 \ge k_2$ ;
      \item $\sigma \models \exists i, A$ si et seulement s'il existe $k \in \mathds{Z}$ tel que $\sigma \models A[\sfrac{k}{i}]$.
    \end{itemize}

    On écrit $\models A$ ("$A$ est valide") lorsque pour tout $\sigma$ tel que $\mathrm{dom}(\sigma) \supseteq \mathsf{vars}(A)$, on a $\sigma \models A$.
  \end{defn}

  \subsection{Règles de la logique de Hoare : dérivabilité des triplets de Hoare.}

  Les triplets de Hoare, notés $\{A\} c \{A'\}$ avec $A$ et $A'$ closes, où $A$ est \textit{précondition}, $c$ est commande $\mathsf{IMP}$, et $A'$ est \textit{postcondition}.
  On définit une relation $\vdash \{A\} c \{A'\}$ sur les triplets de Hoare :
  \begin{gather*}
    \begin{prooftree}
      \hypo{\vdash \{A \land b\} c_1 \{A'\}}
      \hypo{\vdash \{A \land \lnot b\} c_2 \{A'\}}
      \infer 2{\vdash \{A\} \ifte b {c_1} {c_2} \{A'\}}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \infer 0{\vdash \{A\} \mathtt{skip} \{A\}}
    \end{prooftree}\\[1em]
    \begin{prooftree}
      \hypo {\vdash \{A\} c_1 \{A'\} }
      \hypo {\vdash \{A'\} c_2 \{A''\} }
      \infer 2{\vdash \{A\} c_1 \col c_2 \{A''\}}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \hypo{\vdash \{A \land b\} c \{A\}}
      \infer 1{\{A\} \while b c \{A \land \lnot B\}}
    \end{prooftree}\\[1em]
    \begin{prooftree}
      \hypo{\{A\}  c \{A'\}}
      \infer[left label={\begin{array}{l}\models B \implies A\\ \models A' \implies B'\end{array}}] 1{\{B\}  c \{B'\}}
    \end{prooftree}
    \quad\quad
    \begin{prooftree}
      \infer 0{\{A[\sfrac{a}{x}]\} x \ceq a \{A\} }
    \end{prooftree}
  \end{gather*}
  La dernière règle semble à l'envers, mais c'est parce que la logique de Hoare fonctionne fondamentalement à l'envers.

  Dans la règle de dérivation pour la boucle \texttt{while}, l'assertion manipulée, $A$, est un \textit{invariant}.

  L'avant dernière règle s'appelle la \textit{règle de conséquence} : on ne manipule pas le programme, la commande, mais plutôt les pré- et post-conditions.

  La relation $\vdash \{A\} c \{A'\}$ s'appelle la \textit{sémantique opérationnelle} de $\mathsf{IMP}$.

  \begin{defn}
    On définit la relation de \textit{satisfaction}, sur les triplets de la forme $\{A\} c \{A'\}$ avec $A,A'$ closes, avec
    $\sigma \models \{A\}c \{A'\}$ si et seulement si dès lors que $\sigma \models A$ et $c, \sigma \Downarrow \sigma'$ alors on a~$\sigma' \models A'$. 

    On définit ensuite la relation de \textit{validité} par $\models \{A\} c \{A'\}$ si et seulement si pour tout $\sigma \in \mathcal{M}$, $\sigma \models \{A\}c \{A'\}$.
  \end{defn}

  \begin{thm}[Correction de la logique de Hoare.]
    Si $\vdash \{A\} c \{A'\}$ alors $\models \{A\} c \{A'\}$.
  \end{thm}
  \begin{prv}
    On procède par induction sur $\vdash \{A\} c \{A'\}$. Il y a 6 cas.
    \begin{itemize}
      \item \textsl{Règle de conséquence}.
        On sait \[
        \models B \implies A \text{ et }\models A' \implies B'
        ,\]et l'hypothèse d'induction.
        On doit montrer $\models \{B\}c \{B'\}$.
        Soit $\sigma$ tel que $\models B$, et supposons $c, \sigma \Downarrow \sigma'$.
        On a $\models A$ par hypothèse.
        Puis, par hypothèse d'induction, $\sigma' \models A'$ et donc $\sigma' \models B'$.
      \item \textsl{Règle \texttt{while}}.
        Considérons $c = \while b {c_0}$.
        On sait par induction que $\models \{A \land b\}c_0 \{A\}$ et l'hypothèse d'induction.
        Il faut montrer $\models \{A\} \while b {c_0} \{A \land \lnot b\}$, c'est à dire, si $\sigma \models A$ et $(\star) : \while b {c_0}, \sigma \Downarrow \sigma'$ alors $\sigma' \models A \land \lnot b$.
        Pour montrer cela, il est nécessaire de faire une induction sur la dérivation de $(\star)$, "sur le nombre d'itérations dans la boucle".
      \item Autres cas en exercice.
    \end{itemize}
  \end{prv}


  Le sens inverse, la réciproque, s'appelle la \textit{complétude}.
  On l'étudiera rapidement après.

  \begin{rmk}
    Concrètement, on écrit des programmes annotés.
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}[anchor=west,text width=4.5cm]
        \node[color=deepred] (A) {$\{x \ge 1\}$};
        \node[below of=A, color=deepblue] (B) {$\{x \ge 1 \land x + 2 + x + 2 \ge 6\}$};
        \node[below of=B] (C) {$y \ceq x + 2 \col$};
        \node[below of=C, color=deepblue] (D) {$\{x \ge 1 \land y + y \ge 6\}$};
        \node[below of=D] (E) {$z \ceq y + y$};
        \node[below of=E, color=deepblue] (F) {$\{x \ge 1 \land z \ge 6\}$};
        \node[below of=F, color=deepred] (G) {$\{x \ge 1 \land z \ge 6\}$};
        \draw[-implies,deepred,double equal sign distance] (A.west) to[bend right] (B.west);
        \draw[-implies,deepred,double equal sign distance] (F.west) to[bend right] (G.west);
      \end{tikzpicture}
    \end{figure}
  \end{rmk}

  \subsection{Complétude de la logique de Hoare}

  Pour démontrer la complétude de la logique de Hoare, on s'appuie sur la notion de \textit{plus faible précondition} : étant données une commande $c$ et une assertion $B$, alors la \textit{plus faible précondition} associée à $c,B$ est  l'ensemble des états mémoire \[
  \mathrm{wp}(c, B) := \{\sigma  \mid c,\sigma \Downarrow \sigma' \implies \sigma' \models B\}
  .\]
  Ainsi, $\mathrm{wp}(c, B)$ est l'ensemble des états mémoire à partir duquels on aboutit à un état satisfaisant $B$, après une exécution terminante de $c$.

  \begin{prop}
    Pour toute commande $c$ et toute formule $B$, il existe une assertion $\mathrm{W}(c, B)$ telle que $\sigma \models \mathrm{W}(c, B)$ si et seulement si $\sigma \in \mathrm{wp}(c, B)$.
  \end{prop}
  \begin{prv}[esquisse]
    On procède par induction sur $c$.
    Tout fonctionne, sauf pour  \textit{while}\ldots\
    Pour le cas de la boucle \textit{while}, on utilise la caractérisation suivante :
    \begin{gather*}
      \sigma \in \mathrm{wp}(\while b {c_0}, B)\\
      \text{\rotatebox{90}{$\iff$}}\\
      \forall k, \forall \sigma_0, \ldots,\sigma_k \text{ si } \sigma_0 = \sigma \text{ et } \forall i < k, (\sigma_i, b \Downarrow \mathtt{true} \text{ et } c_0, \sigma_i \Downarrow \sigma_{i+1})\\
      \text{ alors }\sigma_k \models b \lor B
    .\end{gather*}
    On peut définir cette assertion en définissant des assertions pour :
    \begin{itemize}
      \item décrire un état mémoire $\sigma_i$ ($X^i_1 = v_1 \land \cdots \land X_n^i = v_n$) ;
      \item exprimer les conditions $\sigma_i, c \Downarrow \sigma_{i+1}$ par induction ;
      \item exprimer les quantifications $\forall k, \sigma_0, \ldots, \sigma_k$\ldots\ on demande à Kurt Gödel.
    \end{itemize}

    Ainsi, on a bien une assertion $\mathrm{W}(c, B)$ telle que \[
    \forall \sigma, \quad\quad \sigma \in \mathrm{wp}(c, B) \iff \sigma \models \mathrm{W}(c,B)
    .\] 
  \end{prv}
\end{document}
